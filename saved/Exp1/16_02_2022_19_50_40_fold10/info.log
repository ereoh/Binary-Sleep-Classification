2022-02-16 19:50:40,540 - train - INFO - AttnSleep(
  (mrcnn): MRCNN(
    (GELU): GELU()
    (features1): Sequential(
      (0): Conv1d(1, 64, kernel_size=(50,), stride=(6,), padding=(24,), bias=False)
      (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): GELU()
      (3): MaxPool1d(kernel_size=8, stride=2, padding=4, dilation=1, ceil_mode=False)
      (4): Dropout(p=0.5, inplace=False)
      (5): Conv1d(64, 128, kernel_size=(8,), stride=(1,), padding=(4,), bias=False)
      (6): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (7): GELU()
      (8): Conv1d(128, 128, kernel_size=(8,), stride=(1,), padding=(4,), bias=False)
      (9): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (10): GELU()
      (11): MaxPool1d(kernel_size=4, stride=4, padding=2, dilation=1, ceil_mode=False)
    )
    (features2): Sequential(
      (0): Conv1d(1, 64, kernel_size=(400,), stride=(50,), padding=(200,), bias=False)
      (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): GELU()
      (3): MaxPool1d(kernel_size=4, stride=2, padding=2, dilation=1, ceil_mode=False)
      (4): Dropout(p=0.5, inplace=False)
      (5): Conv1d(64, 128, kernel_size=(7,), stride=(1,), padding=(3,), bias=False)
      (6): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (7): GELU()
      (8): Conv1d(128, 128, kernel_size=(7,), stride=(1,), padding=(3,), bias=False)
      (9): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (10): GELU()
      (11): MaxPool1d(kernel_size=2, stride=2, padding=1, dilation=1, ceil_mode=False)
    )
    (dropout): Dropout(p=0.5, inplace=False)
    (AFR): Sequential(
      (0): SEBasicBlock(
        (conv1): Conv1d(128, 30, kernel_size=(1,), stride=(1,))
        (bn1): BatchNorm1d(30, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv1d(30, 30, kernel_size=(1,), stride=(1,))
        (bn2): BatchNorm1d(30, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): SELayer(
          (avg_pool): AdaptiveAvgPool1d(output_size=1)
          (fc): Sequential(
            (0): Linear(in_features=30, out_features=1, bias=False)
            (1): ReLU(inplace=True)
            (2): Linear(in_features=1, out_features=30, bias=False)
            (3): Sigmoid()
          )
        )
        (downsample): Sequential(
          (0): Conv1d(128, 30, kernel_size=(1,), stride=(1,), bias=False)
          (1): BatchNorm1d(30, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
    )
  )
  (tce): TCE(
    (layers): ModuleList(
      (0): EncoderLayer(
        (self_attn): MultiHeadedAttention(
          (convs): ModuleList(
            (0): CausalConv1d(30, 30, kernel_size=(7,), stride=(1,), padding=(6,))
            (1): CausalConv1d(30, 30, kernel_size=(7,), stride=(1,), padding=(6,))
            (2): CausalConv1d(30, 30, kernel_size=(7,), stride=(1,), padding=(6,))
          )
          (linear): Linear(in_features=80, out_features=80, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=80, out_features=120, bias=True)
          (w_2): Linear(in_features=120, out_features=80, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (sublayer_output): ModuleList(
          (0): SublayerOutput(
            (norm): LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (1): SublayerOutput(
            (norm): LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (conv): CausalConv1d(30, 30, kernel_size=(7,), stride=(1,), padding=(6,))
      )
      (1): EncoderLayer(
        (self_attn): MultiHeadedAttention(
          (convs): ModuleList(
            (0): CausalConv1d(30, 30, kernel_size=(7,), stride=(1,), padding=(6,))
            (1): CausalConv1d(30, 30, kernel_size=(7,), stride=(1,), padding=(6,))
            (2): CausalConv1d(30, 30, kernel_size=(7,), stride=(1,), padding=(6,))
          )
          (linear): Linear(in_features=80, out_features=80, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=80, out_features=120, bias=True)
          (w_2): Linear(in_features=120, out_features=80, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (sublayer_output): ModuleList(
          (0): SublayerOutput(
            (norm): LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (1): SublayerOutput(
            (norm): LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (conv): CausalConv1d(30, 30, kernel_size=(7,), stride=(1,), padding=(6,))
      )
    )
    (norm): LayerNorm()
  )
  (fc): Linear(in_features=2400, out_features=5, bias=True)
)
2022-02-16 19:56:07,563 - trainer - INFO -     epoch          : 1
2022-02-16 19:56:07,565 - trainer - INFO -     loss           : 0.7457430679707011
2022-02-16 19:56:07,566 - trainer - INFO -     accuracy       : 0.7287412174141441
2022-02-16 19:56:07,567 - trainer - INFO -     val_loss       : 0.8433888501980725
2022-02-16 19:56:07,567 - trainer - INFO -     val_accuracy   : 0.7054181521093286
2022-02-16 20:01:14,250 - trainer - INFO -     epoch          : 2
2022-02-16 20:01:14,251 - trainer - INFO -     loss           : 0.5629363336191056
2022-02-16 20:01:14,252 - trainer - INFO -     accuracy       : 0.8087740700308621
2022-02-16 20:01:14,253 - trainer - INFO -     val_loss       : 0.7171345721272862
2022-02-16 20:01:14,254 - trainer - INFO -     val_accuracy   : 0.7281268568033274
2022-02-16 20:06:15,853 - trainer - INFO -     epoch          : 3
2022-02-16 20:06:15,854 - trainer - INFO -     loss           : 0.5368651538897472
2022-02-16 20:06:15,855 - trainer - INFO -     accuracy       : 0.8179221694628669
2022-02-16 20:06:15,856 - trainer - INFO -     val_loss       : 0.7208577639916364
2022-02-16 20:06:15,857 - trainer - INFO -     val_accuracy   : 0.744940210932858
2022-02-16 20:11:17,966 - trainer - INFO -     epoch          : 4
2022-02-16 20:11:17,967 - trainer - INFO -     loss           : 0.5188673498334399
2022-02-16 20:11:17,968 - trainer - INFO -     accuracy       : 0.8247553495797492
2022-02-16 20:11:17,968 - trainer - INFO -     val_loss       : 0.7388861757867476
2022-02-16 20:11:17,969 - trainer - INFO -     val_accuracy   : 0.7147764408793821
2022-02-16 20:16:17,022 - trainer - INFO -     epoch          : 5
2022-02-16 20:16:17,023 - trainer - INFO -     loss           : 0.5095081373954274
2022-02-16 20:16:17,024 - trainer - INFO -     accuracy       : 0.8254827857213213
2022-02-16 20:16:17,024 - trainer - INFO -     val_loss       : 0.6028938381110921
2022-02-16 20:16:17,025 - trainer - INFO -     val_accuracy   : 0.7801916221033869
2022-02-16 20:21:22,880 - trainer - INFO -     epoch          : 6
2022-02-16 20:21:22,881 - trainer - INFO -     loss           : 0.48943119140187646
2022-02-16 20:21:22,882 - trainer - INFO -     accuracy       : 0.8359346784916935
2022-02-16 20:21:22,883 - trainer - INFO -     val_loss       : 0.7284528981236851
2022-02-16 20:21:22,884 - trainer - INFO -     val_accuracy   : 0.7415886809269162
2022-02-16 20:26:36,572 - trainer - INFO -     epoch          : 7
2022-02-16 20:26:36,573 - trainer - INFO -     loss           : 0.4855624012127044
2022-02-16 20:26:36,574 - trainer - INFO -     accuracy       : 0.8343105157101582
2022-02-16 20:26:36,575 - trainer - INFO -     val_loss       : 0.5932293806005927
2022-02-16 20:26:36,576 - trainer - INFO -     val_accuracy   : 0.7924325980392156
2022-02-16 20:31:46,071 - trainer - INFO -     epoch          : 8
2022-02-16 20:31:46,073 - trainer - INFO -     loss           : 0.47833267755948816
2022-02-16 20:31:46,074 - trainer - INFO -     accuracy       : 0.83522314539694
2022-02-16 20:31:46,074 - trainer - INFO -     val_loss       : 0.570080825511147
2022-02-16 20:31:46,075 - trainer - INFO -     val_accuracy   : 0.7838680926916222
2022-02-16 20:36:52,347 - trainer - INFO -     epoch          : 9
2022-02-16 20:36:52,349 - trainer - INFO -     loss           : 0.4719384583128486
2022-02-16 20:36:52,350 - trainer - INFO -     accuracy       : 0.8383311650469499
2022-02-16 20:36:52,351 - trainer - INFO -     val_loss       : 0.6289745551698348
2022-02-16 20:36:52,351 - trainer - INFO -     val_accuracy   : 0.7755960338680927
2022-02-16 20:42:00,016 - trainer - INFO -     epoch          : 10
2022-02-16 20:42:00,018 - trainer - INFO -     loss           : 0.4705797617033029
2022-02-16 20:42:00,020 - trainer - INFO -     accuracy       : 0.8402898048131854
2022-02-16 20:42:00,021 - trainer - INFO -     val_loss       : 0.6128952809992958
2022-02-16 20:42:00,022 - trainer - INFO -     val_accuracy   : 0.7829164809863339
